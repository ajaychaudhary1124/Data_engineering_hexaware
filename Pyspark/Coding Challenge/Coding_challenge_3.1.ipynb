{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5266185-419f-4e36-a073-663d9b2ce304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/12 11:14:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 50000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "| Chandu|Data Science| 45000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"PySpark_codingChallenge\").getOrCreate()\n",
    "df_pyspark=spark.read.csv(\"/Users/ajaychaudhary/jupyter/PySpark/test.csv\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43d37d6-eeac-4e98-8eff-0e6cea75ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     195000|\n",
      "|    Big Data|     285000|\n",
      "|Data Science|     165000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#groupby using sum() function\n",
    "df_pyspark.groupBy(\"department\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7656ea0f-90c9-46cd-901f-453e3931aff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|min(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      60000|\n",
      "|    Big Data|      55000|\n",
      "|Data Science|      45000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24109217-0e09-4b6f-a98c-3470f93417fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|max(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      75000|\n",
      "|    Big Data|      85000|\n",
      "|Data Science|      70000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf5911f-43ee-481d-8ee5-c412847ff3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|    65000.0|\n",
      "|    Big Data|    71250.0|\n",
      "|Data Science|    55000.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c757647-3e50-4cad-be1a-4357ac1836fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|    65000.0|\n",
      "|    Big Data|    71250.0|\n",
      "|Data Science|    55000.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276944c7-4ec9-473a-af82-12f32006a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    3|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713bcdb8-04f8-4ae0-9921-a0a2fb234e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "|  department|Chandu|Krishna|Rashmi|Rashmi |Rohit|Rohit |Ronit|\n",
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "|         IOT|  NULL|   NULL|  NULL|  75000|60000|  NULL|60000|\n",
      "|    Big Data|  NULL|  65000| 85000|   NULL|55000| 80000| NULL|\n",
      "|Data Science| 95000|   NULL|  NULL|   NULL| NULL|  NULL|70000|\n",
      "+------------+------+-------+------+-------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"department\").pivot(\"Name\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab88789-7513-47c3-8f42-1c9307c6c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 45000|\n",
      "| Chandu|Data Science| 50000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.sort(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a580d7a-6800-46e2-b37b-0c37969a2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   name|  department|salary|\n",
      "+-------+------------+------+\n",
      "| Chandu|Data Science| 45000|\n",
      "| Chandu|Data Science| 50000|\n",
      "|  Rohit|    Big Data| 55000|\n",
      "|  Ronit|         IOT| 60000|\n",
      "|  Rohit|         IOT| 60000|\n",
      "|Krishna|    Big Data| 65000|\n",
      "|  Ronit|Data Science| 70000|\n",
      "|Rashmi |         IOT| 75000|\n",
      "| Rohit |    Big Data| 80000|\n",
      "| Rashmi|    Big Data| 85000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alternatively orderBy can be used to sort based on single column\n",
    "df_pyspark.orderBy(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17bba6c-3efb-47ff-a557-286991116fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    name| age|experience|salary|\n",
      "+--------+----+----------+------+\n",
      "|   Ravi |  28|         5| 60000|\n",
      "|  Priya |  35|      NULL| 75000|\n",
      "|   Arun |  40|        12| 85000|\n",
      "|   Neha |NULL|      NULL| 55000|\n",
      "| Suresh |  45|        18| 90000|\n",
      "|Deepika |NULL|      NULL| 65000|\n",
      "| Rajesh |  30|         8| 70000|\n",
      "|  Meera |NULL|      NULL| 50000|\n",
      "| Sanjay |  38|        15| 80000|\n",
      "|  Anita |NULL|        10| 60000|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for dropping\n",
    "df_pyspark1=spark.read.csv(\"/Users/ajaychaudhary/jupyter/PySpark/null_values_test.csv\",header=True,inferSchema=True)\n",
    "df_pyspark1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6bf756a-3312-43ca-8a12-542d9db4e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   name|age|experience|salary|\n",
      "+-------+---+----------+------+\n",
      "|  Ravi | 28|         5| 60000|\n",
      "|  Arun | 40|        12| 85000|\n",
      "|Suresh | 45|        18| 90000|\n",
      "|Rajesh | 30|         8| 70000|\n",
      "|Sanjay | 38|        15| 80000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping rows based on null values\n",
    "df_pyspark1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cdf1e57-be4e-4af3-b72b-194e6c5349d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    name| age|experience|salary|\n",
      "+--------+----+----------+------+\n",
      "|   Ravi |  28|         5| 60000|\n",
      "|  Priya |  35|      NULL| 75000|\n",
      "|   Arun |  40|        12| 85000|\n",
      "|   Neha |NULL|      NULL| 55000|\n",
      "| Suresh |  45|        18| 90000|\n",
      "|Deepika |NULL|      NULL| 65000|\n",
      "| Rajesh |  30|         8| 70000|\n",
      "|  Meera |NULL|      NULL| 50000|\n",
      "| Sanjay |  38|        15| 80000|\n",
      "|  Anita |NULL|        10| 60000|\n",
      "+--------+----+----------+------+\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    name|age|experience|salary|\n",
      "+--------+---+----------+------+\n",
      "|   Ravi | 28|         5| 60000|\n",
      "|  Priya | 35|         0| 75000|\n",
      "|   Arun | 40|        12| 85000|\n",
      "|   Neha |  0|         0| 55000|\n",
      "| Suresh | 45|        18| 90000|\n",
      "|Deepika |  0|         0| 65000|\n",
      "| Rajesh | 30|         8| 70000|\n",
      "|  Meera |  0|         0| 50000|\n",
      "| Sanjay | 38|        15| 80000|\n",
      "|  Anita |  0|        10| 60000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filling missing values with 0\n",
    "df_pyspark1.na.fill('Missing Values').show()\n",
    "df_pyspark1.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51413dab-2eec-4405-9216-982394964126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
